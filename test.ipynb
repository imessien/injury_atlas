{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scgpt.readthedocs.io/en/latest/tutorial_perturbation.html\n",
    "https://huggingface.co/ctheodoris/Geneformer/blob/main/examples/in_silico_perturbation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from rich import print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"/cis/net/r41/data/iessien1/Multi_Injury_Atlas.h5ad\")\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scanpy import tl\n",
    "# 1. Create the combined stratification key\n",
    "# Create stratification key and filter out rare combinations\n",
    "# Count unique categories in finalannotationv1 and strat before filtering\n",
    "# Check for the error: The least populated class in y has only 1 member\n",
    "n_annotations = len(adata.obs['finalannotationv1'].unique())\n",
    "print(f\"Number of unique cell type annotations: {n_annotations}\")\n",
    "\n",
    "# Create strat column temporarily to check unique combinations\n",
    "adata.obs['strat_temp'] = adata.obs['finalannotationv1'].astype(str) + \"_\" + adata.obs['Condition'].astype(str)\n",
    "n_strat = len(adata.obs['strat_temp'].unique())\n",
    "print(f\"Number of unique cell type-condition combinations: {n_strat}\")\n",
    "strat_counts = adata.obs['strat_temp'].value_counts()\n",
    "print(strat_counts)\n",
    "\n",
    "# Filter out combinations with fewer than 2 members to avoid the error\n",
    "valid_strata = strat_counts[strat_counts >= 2].index\n",
    "valid_cells = adata.obs['strat_temp'].isin(valid_strata)\n",
    "adata_filtered = adata[valid_cells].copy()\n",
    "print(f\"Number of filtered cell type-condition combinations: {len(valid_strata)}\")\n",
    "\n",
    "target_genes = ['MYC', 'AKT1', 'CCND1', 'STAT3', 'HSPA5', 'JUN', 'FOS', 'COX4I1', 'HIF1A', 'HSPA9']\n",
    "\n",
    "\n",
    "def eval_perturb(adata, target_genes, cell_type_key='finalannotationv1', condition_key='Condition'):\n",
    "    \"\"\"\n",
    "    Evaluate gene perturbation effects across different cell types and conditions.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object containing single-cell data\n",
    "        target_genes: List of genes to analyze for perturbation\n",
    "        cell_type_key: Key in adata.obs for cell type annotations\n",
    "        condition_key: Key in adata.obs for condition annotations\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with perturbation results\n",
    "    \"\"\"\n",
    "    # Filter for valid genes (those present in the dataset)\n",
    "    valid_genes = [gene for gene in target_genes if gene in adata.var_names]\n",
    "    if len(valid_genes) < len(target_genes):\n",
    "        print(f\"Warning: {len(target_genes) - len(valid_genes)} genes not found in dataset\")\n",
    "    \n",
    "    # Create a dataset for perturbation analysis\n",
    "    # Filter to cells with valid strata (cell type + condition combinations)\n",
    "    valid_cells = adata.obs['strat_temp'].isin(valid_strata)\n",
    "    adata_filtered = adata[valid_cells].copy()\n",
    "    \n",
    "    # Prepare data for train/val/test split\n",
    "    X = adata_filtered.X\n",
    "    strat = adata_filtered.obs['strat_temp'].values  # Convert to numpy array to avoid FutureWarning\n",
    "    \n",
    "    # Display stratification information\n",
    "    print(\"Stratification counts before splitting:\")\n",
    "    strat_df = pd.DataFrame({'strat': strat})\n",
    "    print(strat_df['strat'].value_counts().head(10))\n",
    "    \n",
    "    # First split into train and temp (val+test combined)\n",
    "    train_idx, temp_idx = train_test_split(\n",
    "        np.arange(adata_filtered.n_obs),\n",
    "        test_size=0.3,  # 30% for validation and test combined\n",
    "        random_state=42,\n",
    "        stratify=strat\n",
    "    )\n",
    "    \n",
    "    # Then split temp into validation and test\n",
    "    # Use iloc to access by position in the numpy array\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        temp_idx,\n",
    "        test_size=0.33,  # 1/3 of the 30% (10% of total) for test\n",
    "        random_state=42,\n",
    "        stratify=strat[temp_idx]  # Using numpy array indexing\n",
    "    )\n",
    "    \n",
    "    # Create train, validation, and test datasets\n",
    "    train_data = adata_filtered[train_idx].copy()\n",
    "    val_data = adata_filtered[val_idx].copy()\n",
    "    test_data = adata_filtered[test_idx].copy()\n",
    "    \n",
    "    # Verify stratification worked correctly\n",
    "    print(\"\\nStratification distribution in splits:\")\n",
    "    print(f\"Training data ({train_data.n_obs} cells):\")\n",
    "    print(train_data.obs['strat_temp'].value_counts().head(5))\n",
    "    print(f\"\\nValidation data ({val_data.n_obs} cells):\")\n",
    "    print(val_data.obs['strat_temp'].value_counts().head(5))\n",
    "    print(f\"\\nTest data ({test_data.n_obs} cells):\")\n",
    "    print(test_data.obs['strat_temp'].value_counts().head(5))\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "# Create train, validation, and test datasets for perturbation analysis\n",
    "train_data, val_data, test_data = eval_perturb(adata, target_genes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a dataset class for gene perturbation\n",
    "class GenePerturbationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, adata, tokenizer, max_length=2048):\n",
    "        self.adata = adata\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Extract gene expression data\n",
    "        if isinstance(adata.X, np.ndarray):\n",
    "            self.expression = adata.X\n",
    "        else:\n",
    "            self.expression = adata.X.toarray()\n",
    "        \n",
    "        # Get gene names\n",
    "        self.gene_names = list(adata.var_names)\n",
    "        \n",
    "        # Create mapping from gene names to indices\n",
    "        self.gene_to_idx = {gene: idx for idx, gene in enumerate(self.gene_names)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.adata.n_obs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get expression vector for this cell\n",
    "        expr = self.expression[idx]\n",
    "        \n",
    "        # Convert to tokens using the tokenizer\n",
    "        # We only include expressed genes (non-zero values)\n",
    "        expressed_genes = [self.gene_names[i] for i in np.where(expr > 0)[0]]\n",
    "        \n",
    "        # Tokenize the gene names\n",
    "        tokens = self.tokenizer(\n",
    "            expressed_genes,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokens.input_ids.squeeze(),\n",
    "            \"attention_mask\": tokens.attention_mask.squeeze(),\n",
    "            \"expression\": torch.tensor(expr, dtype=torch.float32)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import optuna\n",
    "import os\n",
    "from Geneformer import InSilicoPerturber, EmbExtractor\n",
    "\n",
    "# Define target genes for analysis\n",
    "target_genes = ['MYC', 'AKT1', 'CCND1', 'STAT3', 'HSPA5', 'JUN', 'FOS', 'COX4I1', 'HIF1A', 'HSPA9']\n",
    "\n",
    "def train_and_evaluate_model(model_name, train_data, val_data, cell_states_to_model=None, test_data=None, epochs=50) -> Tuple[sc.AnnData, float, float]:\n",
    "    \"\"\"Train model and evaluate predictions on both validation and holdout sets.\"\"\"\n",
    "    try:\n",
    "        if model_name == \"Geneformer\":\n",
    "            # Define output directory for saving models and hyperparameters\n",
    "            output_base_dir = \"/cis/net/r41/data/iessien1/\"\n",
    "            os.makedirs(output_base_dir, exist_ok=True)\n",
    "            \n",
    "            # Initialize tokenizer and dataset\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"ctheodoris/Geneformer\")\n",
    "            train_dataset = GenePerturbationDataset(train_data, tokenizer)\n",
    "            val_dataset = GenePerturbationDataset(val_data, tokenizer)\n",
    "            test_dataset = GenePerturbationDataset(test_data, tokenizer) if test_data is not None else None\n",
    "            \n",
    "            # Setup data loaders\n",
    "            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=32) if test_data is not None else None\n",
    "            \n",
    "            # Define custom data collator to handle gene expression data\n",
    "            def data_collator(features):\n",
    "                input_ids = torch.stack([f[\"input_ids\"] for f in features])\n",
    "                attention_mask = torch.stack([f[\"attention_mask\"] for f in features])\n",
    "                labels = torch.stack([f[\"expression\"] for f in features])\n",
    "                return {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"labels\": labels\n",
    "                }\n",
    "            \n",
    "            # Initialize model with hyperparameter optimization\n",
    "            def objective(trial):\n",
    "                # Define hyperparameters to tune\n",
    "                learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "                weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "                batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "                \n",
    "                # Initialize model\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                    \"ctheodoris/Geneformer\",\n",
    "                    num_labels=len(target_genes)\n",
    "                )\n",
    "                \n",
    "                # Define training arguments with early stopping\n",
    "                trial_output_dir = os.path.join(output_base_dir, f\"trial_{trial.number}\")\n",
    "                training_args = TrainingArguments(\n",
    "                    output_dir=trial_output_dir,\n",
    "                    learning_rate=learning_rate,\n",
    "                    weight_decay=weight_decay,\n",
    "                    per_device_train_batch_size=batch_size,\n",
    "                    per_device_eval_batch_size=batch_size,\n",
    "                    num_train_epochs=epochs,\n",
    "                    evaluation_strategy=\"epoch\",\n",
    "                    save_strategy=\"epoch\",\n",
    "                    load_best_model_at_end=True,\n",
    "                    metric_for_best_model=\"eval_loss\",\n",
    "                    greater_is_better=False,\n",
    "                    save_total_limit=3,  # Keep only the 3 best checkpoints\n",
    "                    early_stopping_patience=5,  # Stop if no improvement for 5 evaluations\n",
    "                )\n",
    "                \n",
    "                # Define trainer\n",
    "                trainer = Trainer(\n",
    "                    model=model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=train_dataset,\n",
    "                    eval_dataset=val_dataset,  # Use validation set for model selection\n",
    "                    data_collator=data_collator,\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                trainer.train()\n",
    "                \n",
    "                # Evaluate model on validation set\n",
    "                eval_results = trainer.evaluate()\n",
    "                return eval_results[\"eval_loss\"]\n",
    "            \n",
    "            # Run hyperparameter optimization using validation set\n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials=10)\n",
    "            \n",
    "            # Get best hyperparameters\n",
    "            best_params = study.best_params\n",
    "            print(f\"Best hyperparameters: {best_params}\")\n",
    "            \n",
    "            # Save hyperparameters to file\n",
    "            import json\n",
    "            with open(os.path.join(output_base_dir, \"best_hyperparameters.json\"), \"w\") as f:\n",
    "                json.dump(best_params, f, indent=4)\n",
    "            \n",
    "            # Train final model with best hyperparameters\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"ctheodoris/Geneformer\",\n",
    "                num_labels=len(target_genes)\n",
    "            )\n",
    "            \n",
    "            best_model_dir = os.path.join(output_base_dir, \"best_model\")\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=best_model_dir,\n",
    "                learning_rate=best_params[\"learning_rate\"],\n",
    "                weight_decay=best_params[\"weight_decay\"],\n",
    "                per_device_train_batch_size=best_params[\"batch_size\"],\n",
    "                per_device_eval_batch_size=best_params[\"batch_size\"],\n",
    "                num_train_epochs=epochs,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"eval_loss\",\n",
    "                greater_is_better=False,\n",
    "                save_total_limit=3,\n",
    "                early_stopping_patience=5,\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=val_dataset,  # Use validation set for early stopping and model selection\n",
    "                data_collator=data_collator,\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            trainer.train()\n",
    "            \n",
    "            # Save the model locally\n",
    "            model.save_pretrained(os.path.join(output_base_dir, \"finetuned_model\"))\n",
    "            tokenizer.save_pretrained(os.path.join(output_base_dir, \"finetuned_tokenizer\"))\n",
    "            \n",
    "            # Evaluate model on validation set\n",
    "            val_results = trainer.evaluate()\n",
    "            val_loss = val_results[\"eval_loss\"]\n",
    "            \n",
    "            # Evaluate model on test set\n",
    "            test_loss = 0.0\n",
    "            if test_data is not None:\n",
    "                trainer.eval_dataset = test_dataset\n",
    "                test_results = trainer.evaluate()\n",
    "                test_loss = test_results[\"eval_loss\"]\n",
    "            \n",
    "            # For in silico perturbation analysis\n",
    "            if cell_states_to_model is not None:\n",
    "                # Define filter data dictionary for cell types of interest\n",
    "                filter_data_dict = {\"cell_type\": train_data.obs[\"cell_type\"].unique().tolist()}\n",
    "                \n",
    "                # First obtain start, goal, and alt embedding positions using EmbExtractor\n",
    "                embex = EmbExtractor(\n",
    "                    model_type=\"CellClassifier\",  # using fine-tuned cell classifier model\n",
    "                    num_classes=len(target_genes),\n",
    "                    filter_data=filter_data_dict,\n",
    "                    max_ncells=1000,\n",
    "                    emb_layer=0,\n",
    "                    summary_stat=\"exact_mean\",\n",
    "                    forward_batch_size=256,\n",
    "                    nproc=16\n",
    "                )\n",
    "                \n",
    "                # Get state embeddings dictionary\n",
    "                model_path = os.path.join(output_base_dir, \"finetuned_model\")\n",
    "                input_data_path = os.path.join(output_base_dir, \"input_data\")\n",
    "                output_dir = os.path.join(output_base_dir, \"state_embs\")\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                \n",
    "                state_embs_dict = embex.get_state_embs(\n",
    "                    cell_states_to_model,\n",
    "                    model_path,\n",
    "                    input_data_path,\n",
    "                    output_dir,\n",
    "                    \"state_embs\"\n",
    "                )\n",
    "                \n",
    "                # Setup in silico perturber for gene perturbation analysis\n",
    "                perturber = InSilicoPerturber(\n",
    "                    perturb_type=\"overexpress\",  # or \"delete\" for knockouts\n",
    "                    perturb_rank_shift=None,\n",
    "                    genes_to_perturb=target_genes,\n",
    "                    combos=0,\n",
    "                    anchor_gene=None,\n",
    "                    model_type=\"CellClassifier\",\n",
    "                    num_classes=len(target_genes),\n",
    "                    emb_mode=\"cell\",\n",
    "                    cell_emb_style=\"mean_pool\",\n",
    "                    filter_data=filter_data_dict,\n",
    "                    cell_states_to_model=cell_states_to_model,\n",
    "                    state_embs_dict=state_embs_dict,\n",
    "                    max_ncells=2000,\n",
    "                    emb_layer=0,\n",
    "                    forward_batch_size=400,\n",
    "                    nproc=16\n",
    "                )\n",
    "                \n",
    "                # Run perturbation analysis\n",
    "                perturber.perturb_data(\n",
    "                    model_path,\n",
    "                    input_data_path,\n",
    "                    os.path.join(output_base_dir, \"isp_output\"),\n",
    "                    \"perturbation_results\"\n",
    "                )\n",
    "                \n",
    "                # Store perturbation results in the AnnData object\n",
    "                train_data.uns[\"perturbation_results\"] = os.path.join(output_base_dir, \"isp_output\")\n",
    "            \n",
    "            return train_data, val_loss, test_loss\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} not supported\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in training: {e}\")\n",
    "        return None, float('inf'), float('inf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze perturbation results using InSilicoPerturberStats\n",
    "from Geneformer import InSilicoPerturberStats\n",
    "output_base_dir = \"/cis/net/r41/data/iessien1/\"\n",
    "\n",
    "# Define the cell states to model (same as used in the perturber)\n",
    "# This should match the cell_states_to_model used in InSilicoPerturber\n",
    "cell_states_to_model = {\n",
    "    \"state_key\": \"cell_type\",  # column in metadata that defines cell states\n",
    "    \"start_state\": train_data.obs[\"cell_type\"].unique().tolist()[0],  # first cell type as starting state\n",
    "    \"goal_state\": train_data.obs[\"cell_type\"].unique().tolist()[1],   # second cell type as goal state\n",
    "    \"alt_states\": train_data.obs[\"cell_type\"].unique().tolist()[2:]   # remaining cell types as alternatives\n",
    "}\n",
    "\n",
    "# Initialize the stats analyzer\n",
    "ispstats = InSilicoPerturberStats(\n",
    "    mode=\"goal_state_shift\",  # analyze shifts toward goal state\n",
    "    genes_perturbed=target_genes,  # same genes used in perturbation\n",
    "    combos=0,  # no combinations (matches perturber setting)\n",
    "    anchor_gene=None,  # no anchor gene (matches perturber setting)\n",
    "    cell_states_to_model=cell_states_to_model  # same states as in perturber\n",
    ")\n",
    "\n",
    "# Process the perturbation results\n",
    "isp_output_dir = os.path.join(output_base_dir, \"isp_output\")\n",
    "isp_stats_output_dir = os.path.join(output_base_dir, \"isp_stats\")\n",
    "os.makedirs(isp_stats_output_dir, exist_ok=True)\n",
    "\n",
    "# Extract data from intermediate files and generate final statistics\n",
    "ispstats.get_stats(\n",
    "    isp_output_dir,  # directory with perturbation results\n",
    "    None,  # no token dictionary file specified (using default)\n",
    "    isp_stats_output_dir,  # where to save the stats\n",
    "    \"perturbation_stats\"  # prefix for output files\n",
    ")\n",
    "\n",
    "# Print path to results\n",
    "print(f\"Perturbation statistics saved to: {isp_stats_output_dir}\")\n",
    "\n",
    "# Load and display top genes that shift cell state\n",
    "try:\n",
    "    import pandas as pd\n",
    "    stats_file = os.path.join(isp_stats_output_dir, \"perturbation_stats_goal_state_shift.csv\")\n",
    "    if os.path.exists(stats_file):\n",
    "        stats_df = pd.read_csv(stats_file)\n",
    "        # Display top 10 genes with largest effect\n",
    "        print(\"Top 10 genes with largest effect on cell state shift:\")\n",
    "        print(stats_df.sort_values(by=\"Cosine_sim_mean\", ascending=True).head(10))\n",
    "except Exception as e:\n",
    "    print(f\"Could not load stats file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "injury",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
